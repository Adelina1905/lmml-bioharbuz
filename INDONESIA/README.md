# ğŸ¼ Adversarial Panda Attack

## ğŸ“– Overview

Created an adversarial perturbation to fool an AI classifier into believing a cat/dog image is a panda (â‰¥80% confidence).

## ğŸš€ Quick Start

```bash
# 1. Download dataset
python3 INDONESIA/dataset_import.py

# 2. Train classifier
python3 INDONESIA/train_classifier.py

# 3. Generate perturbation
python3 INDONESIA/main.py
```

## ğŸ“‹ Output

Submit `perturbation.npy` with:
- **Shape**: `(224, 224, 3)`
- **Dtype**: `float32`
- **Range**: `[-0.2, 0.2]`

## ğŸ”§ Method

**PGD Attack with Momentum**:
- 500 iterations
- Step size: 0.01
- Epsilon: 0.2

The algorithm iteratively computes gradients to maximize panda class probability while keeping the perturbation small and imperceptible.

## âœ… Success

**PASS**: Panda confidence â‰¥ 80%  
**FLAG**: `SIGMOID_ADVERSARIAL`

## ğŸ“¦ Requirements

```bash
pip install tensorflow numpy pillow kaggle
```

## ğŸ“ Files

- `main.py` - Generates adversarial perturbation
- `train_classifier.py` - Trains CNN classifier  
- `perturbation.npy` - Output file (submit this)

---

*LMML Hackathon 2025*